name: Wildlife Graduate Assistantship Enhanced Pipeline

on:
  schedule:
    # Weekly on Sundays at 6:00 AM UTC (1:00 AM EST / 12:00 AM CST)
    - cron: '0 6 * * 0'
  workflow_dispatch: # Allow manual trigger for testing
  push:
    branches: [ main ]
    paths: 
      - 'dashboard/**'
      - 'wildlife_job_scraper.py'
      - 'src/**'

jobs:
  enhanced-pipeline-and-deploy:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
      pages: write
      id-token: write
    
    steps:
    - name: 📥 Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: 🌐 Install Chrome Browser
      uses: browser-actions/setup-chrome@v1.7.2
      
    - name: 🚗 Setup ChromeDriver & Display
      run: |
        # Set up virtual display for headless Chrome
        sudo apt-get update
        sudo apt-get install -y xvfb
        export DISPLAY=:99
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
        echo "Virtual display configured for headless Chrome"
        echo "ChromeDriver will be automatically managed by webdriver-manager"
      
    - name: 📦 Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        echo "Dependencies installed successfully"
        
    - name: 🛜️ Scan dependencies for vulnerabilities
      run: |
        pip install pip-audit
        pip-audit --desc --format=json --output=dependency-scan.json || true
        echo "Dependency vulnerability scan completed"
        
    - name: 📁 Create data directories
      run: |
        mkdir -p data/archive
        mkdir -p dashboard/data
        echo "Data directories created"
        
    - name: 🔄 Run enhanced data pipeline
      run: |
        echo "Starting enhanced data pipeline..."
        echo "1) Scraping new jobs from wildlife job board"
        echo "2) Running ML-powered enhanced analysis"
        echo "3) Generating dashboard data with analytics"
        python wildlife_job_scraper.py
        python src/analysis/enhanced_analysis.py
        python src/analysis/enhanced_dashboard_data.py
        echo "Enhanced data pipeline completed successfully"
        
    - name: 📋 Copy data to dashboard directory
      run: |
        # Copy comprehensive data files to dashboard directory for GitHub Pages
        if [ -f "data/verified_graduate_assistantships.json" ]; then
          cp data/verified_graduate_assistantships.json dashboard/
          echo "Copied verified graduate assistantships data ($(jq length data/verified_graduate_assistantships.json) positions)"
        fi
        
        if [ -f "data/verified_graduate_assistantships.json" ]; then
          cp data/verified_graduate_assistantships.json dashboard/export_data.json
          echo "Copied export data (for compatibility)"
        fi
        
        if [ -f "dashboard/enhanced_data.json" ]; then
          echo "Analytics dashboard data ready"
        else
          echo "Warning: Enhanced dashboard data not found"
        fi
        
        echo "Dashboard data files ready"
        
    - name: 🗄️ Archive data with timestamp
      run: |
        timestamp=$(date +"%Y%m%d_%H%M%S")
        
        # Archive historical data files as snapshots
        if [ -f "data/historical_positions.json" ]; then
          cp data/historical_positions.json "data/archive/historical_positions_${timestamp}.json"
          echo "Archived historical positions snapshot"
        fi
        
        if [ -f "data/verified_graduate_assistantships.json" ]; then
          cp data/verified_graduate_assistantships.json "data/archive/verified_grad_${timestamp}.json"
          echo "Archived verified graduate assistantships snapshot"
        fi
        
        if [ -f "data/enhanced_data.json" ]; then
          cp data/enhanced_data.json "data/archive/enhanced_${timestamp}.json"
          echo "Archived enhanced dashboard data"
        fi
        
        if [ -f "data/enhanced_analysis.json" ]; then
          cp data/enhanced_analysis.json "data/archive/analysis_${timestamp}.json"
          echo "Archived enhanced analysis data"
        fi
        
        echo "Data archived with timestamp: ${timestamp}"
        
    - name: 📝 Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "🤖 Wildlife Scraper Bot"
        
        # Add all changes
        git add .
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          # Create commit message
          git commit -m "🤖 Weekly enhanced pipeline update - $(date +'%Y-%m-%d %H:%M UTC')"
          git push origin main
          echo "Changes committed and pushed to main branch"
        fi
        
    - name: 🚀 Deploy Dashboard to GitHub Pages
      uses: peaceiris/actions-gh-pages@v4
      if: success()
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./dashboard
        publish_branch: gh-pages
        force_orphan: true
        user_name: '🤖 Wildlife Dashboard Bot'
        user_email: 'action@github.com'
        commit_message: '🚀 Deploy wildlife analytics dashboard'
        
    - name: 📈 Report Success
      if: success()
      run: |
        echo "✅ Wildlife Graduate Assistantship Analytics completed successfully!"
        echo "📊 Analytics dashboard deployed to GitHub Pages"
        echo "🔗 Available at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/analytics_dashboard.html"
        echo "📁 Historical files updated with new data"
        
    - name: 🚨 Report Failure
      if: failure()
      run: |
        echo "❌ Wildlife enhanced pipeline failed!"
        echo "Check the logs above for detailed error information"
        exit 1